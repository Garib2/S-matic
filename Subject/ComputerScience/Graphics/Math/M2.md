---
layout: content
title: Vector Algebra
permalink: /info/CS/Graphics/Math/M2.md/
---

## 1. Vector Basics

### 1.1 Vector Equivalence
- **Equivalence of vectors**: Vectors are **equivalent** if they have the same **direction** and **magnitude**, regardless of **position**.
- Example: Vectors $$ \mathbf{u} $$ and $$ \mathbf{w} $$ are equivalent because they have the same direction and magnitude, even though they are at different locations.
- 
- Analogy: Walking 3 kilometers east from different starting points represents the same **relative displacement**.
- Conclusion: **Position** does not affect the identity of a vector; only its **direction** and **magnitude** matter.

### 1.2 Vector Addition
- **Vector Addition**: Adding two (or more) vectors means combining their directions and magnitudes. This can be visualized as walking a certain distance in one direction (one vector), then immediately walking in another direction for a certain distance (another vector).

- **Example**: If vectors $$ \mathbf{u} $$ and $$ \mathbf{v} $$ represent two stages of a journey, their sum can be represented by walking directly from the starting point to the final destination, which corresponds to the vector $$ \mathbf{w} $$, where $$ \mathbf{u} + \mathbf{v} = \mathbf{w} $$.

- **Head-to-Tail Rule**: To visually add vectors, place them **head-to-tail**. This means the tail of the second vector starts where the first vector ends. The sum is the vector that directly connects the start and end points.

- **Extended Addition**: This idea can be extended to chains of vector addition. For example, $$ \mathbf{s} + \mathbf{t} + \mathbf{u} + \mathbf{v} = \mathbf{w} $$, where the vectors are added one after another.

### 1.3 Vector Subtraction
- **Vector Subtraction**: Subtracting one vector from another means taking a journey in one direction and then going in the reverse of another direction for the same distance.

- **Intuition**: This can be visualized as walking in one direction (vector) and then retracing your steps in the opposite direction (subtracting the second vector). The result is the new vector representing the difference in position.

- **Example**: If vector $$ \mathbf{v} $$ is subtracted from vector $$ \mathbf{u} $$, it corresponds to walking along $$ \mathbf{u} $$ and then reversing the journey of $$ \mathbf{v} $$.

### 1.4 Vector Scaling
- **Scalar Multiplication**: Scalar multiplication (or scaling) involves changing the **length** of a vector without altering its **direction**.

- **Example**: If vector $$ \mathbf{v} $$ is twice the length of vector $$ \mathbf{u} $$ but has the same orientation, this is represented as $$ \mathbf{v} = 2 \mathbf{u} $$.

- **General Form**: Scalar multiplication is represented as $$ \mathbf{v} = \alpha \mathbf{u} $$, where $$ \alpha $$ is any real number. 

- **Negative Scalars**: A negative scalar (e.g., $$ \alpha = -2 $$) would reverse the direction of the vector while scaling its length.

### 1.5 Properties of Vector Addition and Scalar Multiplication
i. **Commutativity**: 
   $$ \mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u} $$
   - The sum of two vectors is the same regardless of the order of the vectors. (See Figure 3.9)

ii. **Associativity**: 
   $$ \mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w} $$
   - The grouping of the vectors does not affect the sum. (See Figure 3.10)

iii. **Distributivity of addition over multiplication**: 
   $$ (\alpha + \beta) \mathbf{u} = \alpha \mathbf{u} + \beta \mathbf{v} $$
   - Scalar addition distributes over scalar multiplication. (See Figure 3.11)

iv. **Distributivity of multiplication over addition**: 
   $$ \alpha (\mathbf{u} + \mathbf{v}) = \alpha \mathbf{u} + \alpha \mathbf{v} $$
   - Scalar multiplication distributes over vector addition. (See Figure 3.12)

## 2. Vector Space
Although we've been discussing vectors as "directed line segments," the more abstract concept of a **vector space** can be formalized as follows:

A **vector space** (over real numbers) consists of a set $$ V $$, whose elements are called **vectors**, and satisfies these properties:

i. **Addition (and subtraction) of vectors is defined**, and the result of addition or subtraction is another vector.

ii. The set $$ V $$ is **closed under linear combinations**: If $$ \mathbf{u}, \mathbf{v} \in V $$ and $$ \alpha, \beta \in \mathbb{R} $$, then $$ \alpha \mathbf{u} + \beta \mathbf{v} \in V $$ as well.

iii. There exists a unique vector $$ \mathbf{0} \in V $$, called the **zero vector**, such that the following properties hold:
   - a. $$ \forall \mathbf{v} \in V, 0 \cdot \mathbf{v} = \mathbf{0} $$, where $$ 0 \in \mathbb{R} $$.
   - b. $$ \forall \mathbf{v} \in V, \mathbf{0} + \mathbf{v} = \mathbf{v} $$.

(Note: These properties align with our intuitive understanding of "directed line segments" as vectors, except we haven't yet discussed multiplication of vectors by vectors. Also, the "closed under linear combinations" includes scalar multiplication, which we've already discussed.)

### 2.1 Span
Given a set of vectors $$ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n $$ in a vector space $$ V $$, the **set of all linear combinations** of these vectors forms an **infinite** set of vectors that comprises another vector space. This space is called the **space spanned** by $$ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n $$.

That is, any vector $$ \mathbf{w} $$ in this space can be written as:
$$ \mathbf{w} = \lambda_1 \mathbf{v}_1 + \lambda_2 \mathbf{v}_2 + \dots + \lambda_n \mathbf{v}_n, $$ 
where $$ \lambda_i \in \mathbb{R} $$.

The set $$ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n $$ is called the **spanning set** for the space $$ S $$.

### Example:
If we have two **nonparallel** vectors $$ \mathbf{u} $$ and $$ \mathbf{v} $$ in three-dimensional space, the space spanned by these two vectors consists of all vectors lying within the plane defined by $$ \mathbf{u} $$ and $$ \mathbf{v} $$ (see Figure 3.13).
