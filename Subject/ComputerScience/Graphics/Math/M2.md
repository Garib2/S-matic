---
layout: content
title: Vector Algebra
permalink: /info/CS/Graphics/Math/M2.md/
---

## 1. Vector Basics

### 1.1 Vector Equivalence
- **Equivalence of vectors**: Vectors are **equivalent** if they have the same **direction** and **magnitude**, regardless of **position**.
- Example: Vectors $$ \mathbf{u} $$ and $$ \mathbf{w} $$ are equivalent because they have the same direction and magnitude, even though they are at different locations.
- Analogy: Walking 3 kilometers east from different starting points represents the same **relative displacement**.
- Conclusion: **Position** does not affect the identity of a vector; only its **direction** and **magnitude** matter.

### 1.2 Vector Addition
- **Vector Addition**: Adding two (or more) vectors means combining their directions and magnitudes. This can be visualized as walking a certain distance in one direction (one vector), then immediately walking in another direction for a certain distance (another vector).

- **Example**: If vectors $$ \mathbf{u} $$ and $$ \mathbf{v} $$ represent two stages of a journey, their sum can be represented by walking directly from the starting point to the final destination, which corresponds to the vector $$ \mathbf{w} $$, where $$ \mathbf{u} + \mathbf{v} = \mathbf{w} $$.

- **Head-to-Tail Rule**: To visually add vectors, place them **head-to-tail**. This means the tail of the second vector starts where the first vector ends. The sum is the vector that directly connects the start and end points.

- **Extended Addition**: This idea can be extended to chains of vector addition. For example, $$ \mathbf{s} + \mathbf{t} + \mathbf{u} + \mathbf{v} = \mathbf{w} $$, where the vectors are added one after another.

### 1.3 Vector Subtraction
- **Vector Subtraction**: Subtracting one vector from another means taking a journey in one direction and then going in the reverse of another direction for the same distance.

- **Intuition**: This can be visualized as walking in one direction (vector) and then retracing your steps in the opposite direction (subtracting the second vector). The result is the new vector representing the difference in position.

- **Example**: If vector $$ \mathbf{v} $$ is subtracted from vector $$ \mathbf{u} $$, it corresponds to walking along $$ \mathbf{u} $$ and then reversing the journey of $$ \mathbf{v} $$.

### 1.4 Vector Scaling
- **Scalar Multiplication**: Scalar multiplication (or scaling) involves changing the **length** of a vector without altering its **direction**.

- **Example**: If vector $$ \mathbf{v} $$ is twice the length of vector $$ \mathbf{u} $$ but has the same orientation, this is represented as $$ \mathbf{v} = 2 \mathbf{u} $$.

- **General Form**: Scalar multiplication is represented as $$ \mathbf{v} = \alpha \mathbf{u} $$, where $$ \alpha $$ is any real number. 

- **Negative Scalars**: A negative scalar (e.g., $$ \alpha = -2 $$) would reverse the direction of the vector while scaling its length.

### 1.5 Properties of Vector Addition and Scalar Multiplication
i. **Commutativity**: 
   $$ \mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u} $$
   - The sum of two vectors is the same regardless of the order of the vectors. (See Figure 3.9)

ii. **Associativity**: 
   $$ \mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w} $$
   - The grouping of the vectors does not affect the sum. (See Figure 3.10)

iii. **Distributivity of addition over multiplication**: 
   $$ (\alpha + \beta) \mathbf{u} = \alpha \mathbf{u} + \beta \mathbf{v} $$
   - Scalar addition distributes over scalar multiplication. (See Figure 3.11)

iv. **Distributivity of multiplication over addition**: 
   $$ \alpha (\mathbf{u} + \mathbf{v}) = \alpha \mathbf{u} + \alpha \mathbf{v} $$
   - Scalar multiplication distributes over vector addition. (See Figure 3.12)

## 2. Vector Space
Although we've been discussing vectors as "directed line segments," the more abstract concept of a **vector space** can be formalized as follows:

A **vector space** (over real numbers) consists of a set $$ V $$, whose elements are called **vectors**, and satisfies these properties:

i. **Addition (and subtraction) of vectors is defined**, and the result of addition or subtraction is another vector.

ii. The set $$ V $$ is **closed under linear combinations**: If $$ \mathbf{u}, \mathbf{v} \in V $$ and $$ \alpha, \beta \in \mathbb{R} $$, then $$ \alpha \mathbf{u} + \beta \mathbf{v} \in V $$ as well.

iii. There exists a unique vector $$ \mathbf{0} \in V $$, called the **zero vector**, such that the following properties hold:
   - a. $$ \forall \mathbf{v} \in V, 0 \cdot \mathbf{v} = \mathbf{0} $$, where $$ 0 \in \mathbb{R} $$.
   - b. $$ \forall \mathbf{v} \in V, \mathbf{0} + \mathbf{v} = \mathbf{v} $$.

(Note: These properties align with our intuitive understanding of "directed line segments" as vectors, except we haven't yet discussed multiplication of vectors by vectors. Also, the "closed under linear combinations" includes scalar multiplication, which we've already discussed.)

### 2.1 Span
Given a set of vectors $$ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n $$ in a vector space $$ V $$, the **set of all linear combinations** of these vectors forms an **infinite** set of vectors that comprises another vector space. This space is called the **space spanned** by $$ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n $$.

That is, any vector $$ \mathbf{w} $$ in this space can be written as:
$$ \mathbf{w} = \lambda_1 \mathbf{v}_1 + \lambda_2 \mathbf{v}_2 + \dots + \lambda_n \mathbf{v}_n, $$ 
where $$ \lambda_i \in \mathbb{R} $$.

The set $$ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n $$ is called the **spanning set** for the space $$ S $$.

### Example:
If we have two **nonparallel** vectors $$ \mathbf{u} $$ and $$ \mathbf{v} $$ in three-dimensional space, the space spanned by these two vectors consists of all vectors lying within the plane defined by $$ \mathbf{u} $$ and $$ \mathbf{v} $$ (see Figure 3.13).

### 2.2 Linear Independence
Suppose we have a set of vectors $$ \mathbf{u} $$ and $$ \mathbf{v} $$ that span some space $$ S $$.

- **Intuition**: In the example from Figure 3.13, we require that the vectors $$ \mathbf{u} $$ and $$ \mathbf{v} $$ are not parallel. If they were parallel, they would only define a **line**, not a **plane**.

- **Redundancy**: Consider the case where we have three vectors $$ \mathbf{u}, \mathbf{v}, \mathbf{w} $$, but $$ \mathbf{w} = \alpha \mathbf{u} $$ (for some scalar $$ \alpha $$). In this case, the three vectors still span the same space, and $$ \mathbf{w} $$ is redundant because it doesn't add new information to the span.

### **Linear Independence and Dependence**:
- A set of vectors $$ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n $$ is **linearly dependent** if there exist scalars $$ \lambda_1, \lambda_2, \dots, \lambda_n $$ (not all zero) such that:
  $$ \lambda_1 \mathbf{v}_1 + \lambda_2 \mathbf{v}_2 + \dots + \lambda_n \mathbf{v}_n = \mathbf{0} $$.

- A set of vectors is **linearly independent** if:
  $$ \lambda_1 \mathbf{v}_1 + \lambda_2 \mathbf{v}_2 + \dots + \lambda_n \mathbf{v}_n = \mathbf{0} $$ 
  **only if** $$ \lambda_1 = 0, \lambda_2 = 0, \dots, \lambda_n = 0 $$.

### **Intuitive Meaning**:
- A set of vectors is **linearly independent** if and only if **no vector in the set** is a linear combination of the others. That is, none of the vectors can be written as a sum of multiples of the others.

### 2.3 Basis, Subspaces, and Dimension
If we have a vector space $$ S $$, then a set of vectors $$ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n $$ is a **basis** for $$ S $$ if:

1. **Linear Independence**: The vectors $$ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n $$ are linearly independent.
2. **Spanning Set**: The vectors $$ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n $$ form a spanning set for $$ S $$.

If we have a vector space $$ V $$ and a set of basis vectors $$ V = \{ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \} $$, the space $$ S $$ spanned by these vectors is called a **subspace** of $$ V $$.

### Dimension of a Subspace:
- The **dimension** of a subspace $$ S $$ is the maximum number of linearly independent vectors in $$ S $$.

### Example:
- In the example shown in Figure 3.13, the vector space is all directed line segments in three-dimensional space (i.e., $$ V = \mathbb{R}^3 $$).
- The basis vectors are the two directed line segments $$ \{ \mathbf{u}, \mathbf{v} \} $$.
- The space $$ S $$ spanned by these vectors is the plane in which $$ \mathbf{u} $$ and $$ \mathbf{v} $$ lie.
- The dimension of $$ S $$ is 2 because there are 2 linearly independent vectors defining the plane.

### Important Points:
- For any given subspace $$ S $$ of $$ V $$, there are **infinitely many spanning sets**.
- In the example, any two nonparallel directed line segments in the plane form a basis for the planar subset of three-dimensional space.

### Linear Combination and Basis:
- Suppose we have a set of linearly independent vectors $$ V = \{ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \} $$ in $$ V $$.
- Any other vector $$ \mathbf{w} $$ in the space spanned by $$ V $$ can be written as a **linear combination**:
  $$ \mathbf{w} = x_1 \mathbf{v}_1 + x_2 \mathbf{v}_2 + \dots + x_n \mathbf{v}_n, $$ 
  where $$ x_i \in \mathbb{R} $$.

- The coefficients $$ x_i $$ are **unique** for a given $$ \mathbf{w} $$, meaning that no two different sets of coefficients can describe the same vector (this uniqueness holds because the vectors are linearly independent).

### Components and Coordinates:
- The coefficients $$ x_i $$ are the **coordinates** of $$ \mathbf{w} $$ relative to the basis vectors $$ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n $$.
- The elements $$ x_i \mathbf{v}_i $$ are called the **components** of $$ \mathbf{w} $$.

### Example (Figure 3.14):
- The vectors $$ \mathbf{u} $$ and $$ \mathbf{v} $$ from Figure 3.13 are linearly independent (not parallel and neither is the zero vector), so they form a basis.
- A vector $$ \mathbf{w} $$ in the space spanned by $$ \mathbf{u} $$ and $$ \mathbf{v} $$ can be written as a linear combination: 
  $$ \mathbf{w} = 3\mathbf{u} + 2\mathbf{v} $$.
  
  - The **coordinates** of $$ \mathbf{w} $$ are $$ x_1 = 3 $$ and $$ x_2 = 2 $$, as shown by the diagram.
  
  - **Uniqueness**: If $$ x_1 \neq 3 $$, no value of $$ x_2 $$ will give the same vector $$ \mathbf{w} $$.

### Formal Proof (Linear Combination Uniqueness):
- **Proposition**: Let $$ V = \{ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \} $$ be a basis in $$ V $$.
  
  Then:
  $$ \sum_{i=1}^{n} c_i \mathbf{v}_i = \mathbf{0} \quad \text{if and only if} \quad c_1 = c_2 = \dots = c_n = 0 $$.



### 2.3 Basis, Subspaces, and Dimension

#### Definition of a Basis:
A set of vectors $$ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n $$ is a **basis** for a vector space $$ S $$ if:
1. The vectors $$ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n $$ are **linearly independent**.
2. The vectors $$ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n $$ form a **spanning set** for $$ S $$.

#### Subspaces:
- If $$ S $$ is the space spanned by a set of basis vectors $$ V = \{ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \} $$, then $$ S $$ is called a **subspace** of $$ V $$.
- The **dimension** of a subspace $$ S $$ is the maximum number of linearly independent vectors in $$ S $$.

#### Example:
- Consider $$ V = \mathbb{R}^3 $$ (all directed line segments in three-dimensional space).
- A basis for $$ S $$ could be $$ \{ \mathbf{u}, \mathbf{v} \} $$, where $$ \mathbf{u} $$ and $$ \mathbf{v} $$ are two directed line segments.
- The subspace $$ S $$ spanned by $$ \{ \mathbf{u}, \mathbf{v} \} $$ is the plane containing $$ \mathbf{u} $$ and $$ \mathbf{v} $$.
- The dimension of $$ S $$ is 2.

#### Spanning Sets:
- There are **infinitely many spanning sets** for any given subspace $$ S $$.
- For example, any two nonparallel vectors in the plane form a basis for that planar subspace.

#### Linear Combinations:
- Any vector $$ \mathbf{w} $$ in the space spanned by $$ V $$ can be written as:
  $$ \mathbf{w} = x_1 \mathbf{v}_1 + x_2 \mathbf{v}_2 + \dots + x_n \mathbf{v}_n, $$ 
  where $$ x_i \in \mathbb{R} $$ are scalars.
- The coefficients $$ x_i $$ are **unique** for a given $$ \mathbf{w} $$ if the vectors are linearly independent.

#### Components and Coordinates:
- The elements $$ x_i \mathbf{v}_i $$ are called the **components** of $$ \mathbf{w} $$.
- The coefficients $$ x_i $$ are the **coordinates** of $$ \mathbf{w} $$.

#### Example of a Basis:
- In Figure 3.14, $$ \mathbf{u} $$ and $$ \mathbf{v} $$ are linearly independent (not parallel and not the zero vector), so they form a basis.
- A vector $$ \mathbf{w} $$ in the space spanned by $$ \mathbf{u} $$ and $$ \mathbf{v} $$ can be written as:
  $$ \mathbf{w} = 3\mathbf{u} + 2\mathbf{v} $$
  - The coordinates of $$ \mathbf{w} $$ are $$ x_1 = 3 $$ and $$ x_2 = 2 $$.

#### Uniqueness of Linear Combination:
- A linear combination is **unique** for a given vector $$ \mathbf{w} $$ if the basis vectors are linearly independent.

#### Formal Proposition:
Let $$ V = \{ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \} $$ be a basis in $$ V $$.
- $$ \sum_{i=1}^n c_i \mathbf{v}_i = \mathbf{0} $$ **if and only if** $$ c_1 = c_2 = \dots = c_n = 0 $$.





### 2.4 Orientation
### Orientation, Angles, and the Right-Hand Rule

#### Plane Defined by Two Vectors:
- Two **linearly independent** vectors $$ \mathbf{u} $$ and $$ \mathbf{v} $$ define a plane. If the vectors are in three-dimensional space, they define a plane in 3-space.
- **Angle Between Vectors**: The angle $$ \theta_{\mathbf{u}, \mathbf{v}} $$ is measured between $$ \mathbf{u} $$ and $$ \mathbf{v} $$ in lexicographic order, and the arrow for $$ \theta_{\mathbf{u}, \mathbf{v}} $$ is drawn **counterclockwise**.

#### Conventions:
- **Counterclockwise Direction**: This is based on a "page convention," where the plane is drawn on a page, and "out of the page" is a well-defined third dimension.
- **Ambiguity**: If we view the plane from the opposite side, the sense of "counterclockwise" reverses, highlighting that orientation depends on how we define the third direction.

#### Adding a Third Vector:
- Introducing a third **linearly independent** vector $$ \mathbf{w} $$ allows us to define the **orientation** of the basis:
  - $$ \mathbf{w} $$ represents the "out of the page" direction.
  - $$ \mathbf{w} $$ can also define orientation relative to $$ \mathbf{u} $$ and $$ \mathbf{v} $$, or vice versa.

#### Defining the Sign of a Basis:
- The sign of a basis is denoted as:
  $$ \text{sgn}(\mathbf{u}, \mathbf{v}, \mathbf{w}) $$
- The orientation is determined by the direction of $$ \mathbf{w} $$:
  - **Positive Orientation**: $$ \text{sgn}(\mathbf{u}, \mathbf{v}, \mathbf{w}) = +1 $$.
  - The convention for positive orientation is based on the **right-hand rule**.

#### The Right-Hand Rule:
- To determine the orientation:
  1. Point the fingers of your right hand in the direction of $$ \mathbf{u} $$.
  2. Curl your fingers toward $$ \mathbf{v} $$ (the direction of positive rotation).
  3. Your thumb points in the direction of $$ \mathbf{w} $$ (the "out of the page" direction).
- This convention aligns with our intuitive sense of "front" and "back" on a sheet of paper.

#### Visualization:
- In Figure 3.15:
  - $$ \mathbf{w} $$ points out of or into the page, depending on the chosen convention.
  - Choosing $$ \mathbf{w} $$ to point out of the page corresponds to a positive orientation.
- In Figure 3.16:
  - The **right-hand rule** visually illustrates the positive orientation.

